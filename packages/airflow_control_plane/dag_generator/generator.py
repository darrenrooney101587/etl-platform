"""Airflow DAG generator for discovered ETL jobs.

This module generates Airflow DAG definitions from discovered jobs,
creating Python files that Airflow can load and schedule.
"""
from __future__ import annotations

import logging
import os
from pathlib import Path
from typing import List

from airflow_control_plane.discovery.scanner import DiscoveredJob

logger = logging.getLogger(__name__)


class DagGenerator:
    """Generates Airflow DAG files from discovered jobs."""
    
    def __init__(self, dags_output_dir: str):
        """Initialize the DAG generator.
        
        Args:
            dags_output_dir: Directory where generated DAG files will be written.
        """
        self.dags_output_dir = Path(dags_output_dir)
        self.dags_output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_dag_for_job(self, job: DiscoveredJob) -> str:
        """Generate a DAG definition for a single job.
        
        Args:
            job: The discovered job to generate a DAG for.
        
        Returns:
            The generated DAG Python code as a string.
        """
        dag_id = f"{job.package_name}_{job.job_name}"
        
        # Create a basic DAG template
        dag_code = f'''"""
Auto-generated Airflow DAG for {job.package_name}.{job.job_name}

Description: {job.definition.description}
Generated by: airflow_control_plane
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import sys
import os

# Ensure packages are in Python path
packages_root = os.environ.get('ETL_PACKAGES_ROOT', '/app/packages')
if packages_root not in sys.path:
    sys.path.insert(0, packages_root)


def run_job(**context):
    """Execute the ETL job."""
    import logging
    from {job.module_path} import JOB
    
    logger = logging.getLogger(__name__)
    logger.info("Starting job: {job.package_name}.{job.job_name}")
    
    entrypoint, description = JOB
    
    # Get any arguments from Airflow config or context
    job_args = context.get('dag_run').conf.get('args', []) if context.get('dag_run') else []
    
    try:
        exit_code = entrypoint(job_args)
        if exit_code != 0:
            raise RuntimeError(f"Job exited with code {{exit_code}}")
        logger.info("Job completed successfully")
    except Exception as e:
        logger.error(f"Job failed: {{e}}")
        raise


default_args = {{
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}}

# Create the DAG
dag = DAG(
    dag_id='{dag_id}',
    default_args=default_args,
    description='{job.definition.description}',
    schedule_interval=None,  # Manual trigger by default
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['{job.package_name}', 'etl', 'auto-generated'],
)

# Create the task
task = PythonOperator(
    task_id='run_{job.job_name}',
    python_callable=run_job,
    dag=dag,
)
'''
        return dag_code
    
    def write_dag_file(self, job: DiscoveredJob, dag_code: str) -> Path:
        """Write a DAG file to the output directory.
        
        Args:
            job: The job for which the DAG was generated.
            dag_code: The generated DAG Python code.
        
        Returns:
            Path to the written DAG file.
        """
        dag_filename = f"{job.package_name}_{job.job_name}_dag.py"
        dag_filepath = self.dags_output_dir / dag_filename
        
        with open(dag_filepath, 'w') as f:
            f.write(dag_code)
        
        logger.info("Generated DAG file: %s", dag_filepath)
        return dag_filepath
    
    def generate_all_dags(self, jobs: List[DiscoveredJob]) -> List[Path]:
        """Generate DAG files for all discovered jobs.
        
        Args:
            jobs: List of discovered jobs to generate DAGs for.
        
        Returns:
            List of paths to generated DAG files.
        """
        generated_files = []
        
        for job in jobs:
            try:
                dag_code = self.generate_dag_for_job(job)
                filepath = self.write_dag_file(job, dag_code)
                generated_files.append(filepath)
            except Exception as e:
                logger.error(
                    "Failed to generate DAG for %s.%s: %s",
                    job.package_name,
                    job.job_name,
                    e,
                )
        
        logger.info("Generated %d DAG files", len(generated_files))
        return generated_files
    
    def clean_stale_dags(self, current_jobs: List[DiscoveredJob]) -> None:
        """Remove DAG files for jobs that no longer exist.
        
        Args:
            current_jobs: List of currently discovered jobs.
        """
        current_dag_filenames = {
            f"{job.package_name}_{job.job_name}_dag.py"
            for job in current_jobs
        }
        
        for dag_file in self.dags_output_dir.glob("*_dag.py"):
            if dag_file.name not in current_dag_filenames:
                logger.info("Removing stale DAG file: %s", dag_file)
                dag_file.unlink()
